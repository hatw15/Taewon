{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GeMopuySaLF"
      },
      "outputs": [],
      "source": [
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Arc2Face 모델\n",
        "hf_hub_download(\"FoivosPar/Arc2Face\", \"arc2face/config.json\",           local_dir=\"models/arc2face\")\n",
        "hf_hub_download(\"FoivosPar/Arc2Face\", \"arc2face/diffusion_pytorch_model.safetensors\", local_dir=\"models/arc2face\")\n",
        "hf_hub_download(\"FoivosPar/Arc2Face\", \"encoder/config.json\",             local_dir=\"models/encoder\")\n",
        "hf_hub_download(\"FoivosPar/Arc2Face\", \"encoder/pytorch_model.bin\",       local_dir=\"models/encoder\")\n",
        "\n",
        "# AntelopeV2 + ArcFace ONNX\n",
        "!mkdir -p models/antelopev2\n",
        "# (zip 해제 or hf_hub_download로 arcface.onnx까지 models/antelopev2에 위치)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pLKyFzf7UihG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab에서 실행\n",
        "!mkdir -p models/antelopev2\n",
        "!unzip \"/content/drive/MyDrive/antelopev2.zip\" -d models/antelopev2"
      ],
      "metadata": {
        "id": "MvI-22k4Uorg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_hub_download(repo_id=\"FoivosPar/Arc2Face\", filename=\"arcface.onnx\", local_dir=\"./models/antelopev2\")"
      ],
      "metadata": {
        "id": "TDvYjs5dVBUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade \\\n",
        "  \"huggingface_hub==0.17.3\" \\\n",
        "  \"transformers==4.41.0\" \\\n",
        "  \"diffusers==0.23.0\" \\\n",
        "  \"accelerate==0.20.3\"\n"
      ],
      "metadata": {
        "id": "P322_Qnc6nKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import (\n",
        "    StableDiffusionPipeline,\n",
        "    UNet2DConditionModel,\n",
        "    DPMSolverMultistepScheduler,\n",
        ")\n",
        "import sys, os\n",
        "# Arc2Face 클론 경로에 맞게 변경\n",
        "sys.path.append(os.path.abspath(\"/content/Arc2Face\"))\n",
        "\n",
        "\n",
        "from arc2face import CLIPTextModelWrapper, project_face_embs\n",
        "\n",
        "import torch\n",
        "from insightface.app import FaceAnalysis\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Arc2Face is built upon SD1.5\n",
        "# The repo below can be used instead of the now deprecated 'runwayml/stable-diffusion-v1-5'\n",
        "base_model = 'stable-diffusion-v1-5/stable-diffusion-v1-5'\n",
        "\n",
        "encoder = CLIPTextModelWrapper.from_pretrained(\n",
        "    'models', subfolder=\"encoder\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    'models', subfolder=\"arc2face\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "        base_model,\n",
        "        text_encoder=encoder,\n",
        "        unet=unet,\n",
        "        torch_dtype=torch.float16,\n",
        "        safety_checker=None\n",
        "    )"
      ],
      "metadata": {
        "id": "Mxfthz_hVL-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DPMSolverMultistepScheduler\n",
        "from arc2face import CLIPTextModelWrapper\n",
        "import torch\n",
        "\n",
        "# encoder, unet 로드\n",
        "encoder = CLIPTextModelWrapper.from_pretrained('models', subfolder=\"encoder\", torch_dtype=torch.float16)\n",
        "unet    = UNet2DConditionModel.from_pretrained('models', subfolder=\"arc2face\", torch_dtype=torch.float16)\n",
        "\n",
        "# 파이프라인 생성\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "    'stable-diffusion-v1-5',\n",
        "    text_encoder=encoder,\n",
        "    unet=unet,\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ")\n",
        "\n",
        "# 스케줄러 교체 및 CUDA 이동\n",
        "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "pipeline = pipeline.to('cuda')\n"
      ],
      "metadata": {
        "id": "igL9W6wtNzVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) 맞춤 버전 설치: cached_download 도 있고 split_* 도 있는 버전\n",
        "%pip install huggingface_hub==0.25.2\n",
        "\n",
        "# 2) (선택) diffusers, transformers, accelerate 버전 고정\n",
        "%pip install --upgrade diffusers==0.23.0 transformers==4.34.1 accelerate==0.20.3\n"
      ],
      "metadata": {
        "id": "1hjYMrDi85Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DPMSolverMultistepScheduler\n",
        "from arc2face import CLIPTextModelWrapper, project_face_embs\n",
        "\n",
        "# 1) 텍스트 인코더와 U-Net 로드\n",
        "encoder = CLIPTextModelWrapper.from_pretrained(\n",
        "    \"models\", subfolder=\"encoder\", torch_dtype=torch.float16\n",
        ")\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    \"models\", subfolder=\"arc2face\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# 2) Stable Diffusion 기반 Arc2Face 파이프라인 생성\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "    \"stable-diffusion-v1-5\",\n",
        "    text_encoder=encoder,\n",
        "    unet=unet,\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None,\n",
        ")\n",
        "\n",
        "# 3) 스케줄러 교체 및 CUDA로 이동\n",
        "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "pipeline = pipeline.to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "Oa2Lsuao7WrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from insightface.app import FaceAnalysis\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# 1) AntelopeV2 얼굴 분석 객체 준비\n",
        "app = FaceAnalysis(name=\"antelopev2\", root=\".\", providers=[\"CUDAExecutionProvider\",\"CPUExecutionProvider\"])\n",
        "app.prepare(ctx_id=0, det_size=(640,640))\n",
        "\n",
        "# 2) 예시 이미지 로드 -> 가장 큰 얼굴 선택\n",
        "img = np.array(Image.open(\"assets/examples/joacquin.png\"))[:,:,::-1]\n",
        "faces = app.get(img)\n",
        "# bbox 면적 기준 가장 큰 face\n",
        "face = sorted(faces, key=lambda x:(x[\"bbox\"][2]-x[\"bbox\"][0])*(x[\"bbox\"][3]-x[\"bbox\"][1]))[-1]\n",
        "\n",
        "# 3) ArcFace 임베딩 생성 및 정규화\n",
        "id_emb = torch.tensor(face[\"embedding\"], dtype=torch.float16)[None].cuda()\n",
        "id_emb = id_emb / torch.norm(id_emb, dim=1, keepdim=True)\n",
        "\n",
        "# 4) Arc2Face 텍스트 인코더를 통해 ID 토큰 자리에 임베딩 주입\n",
        "prompt_embeds = project_face_embs(pipeline, id_emb)\n"
      ],
      "metadata": {
        "id": "7SuH358n7Xf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 원하는 생성 파라미터\n",
        "num_images = 4\n",
        "steps       = 25\n",
        "guidance    = 3.0\n",
        "\n",
        "# 1) 생성\n",
        "outputs = pipeline(\n",
        "    prompt_embeds=prompt_embeds,\n",
        "    num_inference_steps=steps,\n",
        "    guidance_scale=guidance,\n",
        "    num_images_per_prompt=num_images\n",
        ")\n",
        "\n",
        "# 2) PIL.Image 리스트로 결과 획득\n",
        "images = outputs.images  # length == num_images\n",
        "\n",
        "# 3) 결과 저장 및 표시\n",
        "for i, im in enumerate(images):\n",
        "    fn = f\"/content/output_id{i}.png\"\n",
        "    im.save(fn)\n",
        "    display(im)      # Colab 환경에서 이미지 바로 보기\n",
        "    print(\"saved to\", fn)\n"
      ],
      "metadata": {
        "id": "dZJpecGE7bra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}